{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Test 1: Classification Steel Plates Anomaly Detection Author: Virginia Herrero Email | LinkedIn | GitHub Introducci\u00f3n Este proyecto consiste en un proceso completo de machine learning con el objetivo de predecir el tipo de defecto o anomal\u00eda producido en placas de acero industrial. Este notebook ofrece una gu\u00eda paso a paso de cada etapa del ejercicio, desde el preprocesamiento de datos hasta la evaluaci\u00f3n del modelo, destacando las t\u00e9cnicas y metodolog\u00edas utilizadas a lo largo del camino. Eleg\u00ed este conjunto de datos por su relevancia en la industria 2.0 y en el control de calidad. La detecci\u00f3n de anomal\u00edas en productos industriales es un excelente ejemplo para mostrar mis habilidades en ingenier\u00eda y ciencia de datos. Este proyecto ilustra c\u00f3mo los enfoques basados en datos pueden optimizar la gesti\u00f3n de calidad y la toma de decisiones, siendo aplicables a diversas industrias. Dataset El conjunto de datos utilizado en este proyecto se obtuvo de la p\u00e1gina web Kaggle here . Las variables de este conjunto de datos son las siguientes: X_Minimum X_Maximum Y_Minimum Y_Maximum Pixels_Areas X_Perimeter Y_Perimeter Sum_of_Luminosity Minimum_of_Luminosity Maximum_of_Luminosity Length_of_Conveyer TypeOfSteel_A300 TypeOfSteel_A400 Steel_Plate_Thickness Edges_Index Empty_Index Square_Index Outside_X_Index Edges_X_Index Edges_Y_Index Outside_Global_Index LogOfAreas Log_X_Index Log_Y_Index Orientation_Index Luminosity_Index SigmoidOfAreas Los 7 defectos en las placas de acero son los siguientes: Pastry Z_Scratch K_Scatch Stains Dirtiness Bumps Other_Faults","title":"Home"},{"location":"#test-1-classification","text":"","title":"Test 1: Classification"},{"location":"#steel-plates-anomaly-detection","text":"Author: Virginia Herrero Email | LinkedIn | GitHub","title":"Steel Plates Anomaly Detection"},{"location":"#introduccion","text":"Este proyecto consiste en un proceso completo de machine learning con el objetivo de predecir el tipo de defecto o anomal\u00eda producido en placas de acero industrial. Este notebook ofrece una gu\u00eda paso a paso de cada etapa del ejercicio, desde el preprocesamiento de datos hasta la evaluaci\u00f3n del modelo, destacando las t\u00e9cnicas y metodolog\u00edas utilizadas a lo largo del camino. Eleg\u00ed este conjunto de datos por su relevancia en la industria 2.0 y en el control de calidad. La detecci\u00f3n de anomal\u00edas en productos industriales es un excelente ejemplo para mostrar mis habilidades en ingenier\u00eda y ciencia de datos. Este proyecto ilustra c\u00f3mo los enfoques basados en datos pueden optimizar la gesti\u00f3n de calidad y la toma de decisiones, siendo aplicables a diversas industrias.","title":"Introducci\u00f3n"},{"location":"#dataset","text":"El conjunto de datos utilizado en este proyecto se obtuvo de la p\u00e1gina web Kaggle here . Las variables de este conjunto de datos son las siguientes: X_Minimum X_Maximum Y_Minimum Y_Maximum Pixels_Areas X_Perimeter Y_Perimeter Sum_of_Luminosity Minimum_of_Luminosity Maximum_of_Luminosity Length_of_Conveyer TypeOfSteel_A300 TypeOfSteel_A400 Steel_Plate_Thickness Edges_Index Empty_Index Square_Index Outside_X_Index Edges_X_Index Edges_Y_Index Outside_Global_Index LogOfAreas Log_X_Index Log_Y_Index Orientation_Index Luminosity_Index SigmoidOfAreas Los 7 defectos en las placas de acero son los siguientes: Pastry Z_Scratch K_Scatch Stains Dirtiness Bumps Other_Faults","title":"Dataset"},{"location":"data-cleaning/","text":"Data cleaning Limpia y preprocesa el conjunto de datos antes de seguir con el an\u00e1lisis. df.info() Renombrar columnas Se renombraron algunas columnas para mejorar la legibilidad y la comprensi\u00f3n del conjunto de datos. df.columns df = df.rename(columns = {\"TypeOfSteel_A300\" : \"Steel_Type_A300\", \"TypeOfSteel_A400\" : \"Steel_Type_A400\", \"LogOfAreas\" : \"Log_of_Areas\", \"SigmoidOfAreas\" : \"Sigmoid_of_Areas\", \"K_Scatch\" : \"K_Scratch\"}) df.head() Tipos de datos Verifica que todas las columnas tengan los tipos de datos apropiados. df.dtypes Valores nulos Identifica y elimina cualquier valor nulo en el conjunto de datos cuando sea necesario. # Check the total of null values in each column df.isna().sum() No hay valores nulos en el dataset. Valores duplicados Verifica si hay valores duplicados en el conjunto de datos. df.duplicated().sum() No hay ninguna entrada duplicada en el dataset. Outliers Revisa el resumen estad\u00edstico del conjunto de datos para detectar posibles valores at\u00edpicos. Esta evaluaci\u00f3n inicial permitir\u00e1 identificar cualquier valor inusual que necesite un an\u00e1lisis m\u00e1s detallado. # Changes float format to display two decimals pd.set_option(\"display.float_format\", \"{:.2f}\".format) df.describe().T A primera vista, columnas como X_Minimum , X_Maximum , Y_Minimum , Y_Maximum , Pixel_Areas , X_Perimeter , Y_Perimeter , Sum_of_Luminosity y Steel_plate_thickness pueden contener valores at\u00edpicos. Esta conclusi\u00f3n se basa en la observaci\u00f3n de que los valores m\u00e1ximos superan tanto la media, la mediana, y el tercerl cuartil, lo que puede indicar la presencia de outliers. Dado que estos pueden ser claros indicadores de defectos en las placas de acero, he decidido mantener los valores at\u00edpicos para no reducir las observaciones del conjunto de datos y contribuir a una mejor predicci\u00f3n del modelo. El conjunto de datos limpio: defect_detection = df.copy() defect_detection.head() # Save the cleaned dataset defect_detection.to_csv(\"../data/processed/Steel-plates-faults-cleaned-dataset.csv\")","title":"Data Cleaning"},{"location":"data-cleaning/#data-cleaning","text":"Limpia y preprocesa el conjunto de datos antes de seguir con el an\u00e1lisis. df.info() Renombrar columnas Se renombraron algunas columnas para mejorar la legibilidad y la comprensi\u00f3n del conjunto de datos. df.columns df = df.rename(columns = {\"TypeOfSteel_A300\" : \"Steel_Type_A300\", \"TypeOfSteel_A400\" : \"Steel_Type_A400\", \"LogOfAreas\" : \"Log_of_Areas\", \"SigmoidOfAreas\" : \"Sigmoid_of_Areas\", \"K_Scatch\" : \"K_Scratch\"}) df.head() Tipos de datos Verifica que todas las columnas tengan los tipos de datos apropiados. df.dtypes Valores nulos Identifica y elimina cualquier valor nulo en el conjunto de datos cuando sea necesario. # Check the total of null values in each column df.isna().sum() No hay valores nulos en el dataset. Valores duplicados Verifica si hay valores duplicados en el conjunto de datos. df.duplicated().sum() No hay ninguna entrada duplicada en el dataset. Outliers Revisa el resumen estad\u00edstico del conjunto de datos para detectar posibles valores at\u00edpicos. Esta evaluaci\u00f3n inicial permitir\u00e1 identificar cualquier valor inusual que necesite un an\u00e1lisis m\u00e1s detallado. # Changes float format to display two decimals pd.set_option(\"display.float_format\", \"{:.2f}\".format) df.describe().T A primera vista, columnas como X_Minimum , X_Maximum , Y_Minimum , Y_Maximum , Pixel_Areas , X_Perimeter , Y_Perimeter , Sum_of_Luminosity y Steel_plate_thickness pueden contener valores at\u00edpicos. Esta conclusi\u00f3n se basa en la observaci\u00f3n de que los valores m\u00e1ximos superan tanto la media, la mediana, y el tercerl cuartil, lo que puede indicar la presencia de outliers. Dado que estos pueden ser claros indicadores de defectos en las placas de acero, he decidido mantener los valores at\u00edpicos para no reducir las observaciones del conjunto de datos y contribuir a una mejor predicci\u00f3n del modelo. El conjunto de datos limpio: defect_detection = df.copy() defect_detection.head() # Save the cleaned dataset defect_detection.to_csv(\"../data/processed/Steel-plates-faults-cleaned-dataset.csv\")","title":"Data cleaning"},{"location":"data-exploration/","text":"Exploratory Data Analysis En esta secci\u00f3n, se realiza un an\u00e1lisis exploratorio de datos en profundidad. Target Variable El primer paso es examinar la variable objetivo para obtener una visi\u00f3n general de su distribuci\u00f3n. El objetivo de este proyecto es predecir fallos en las placas de acero. Este conjunto de datos contiene 7 columnas que representan los 7 posibles fallos. Estas 7 columnas son la variable objetivo de este problema. # Plot distribution of all target columns on the datasest fig, ax = plt.subplots(2, 4, figsize = (20, 8)) ax = ax.flatten() for i, col in enumerate(defect_detection.columns[-7:]): sns.histplot(defect_detection[col], ax = ax[i], color = \"#74add1\") Verifica si hay observaciones sin fallos. # List of fault columns faults = [\"Pastry\", \"Z_Scratch\", \"K_Scratch\", \"Stains\", \"Dirtiness\", \"Bumps\", \"Other_Faults\"] # Get the observations with no faults no_faults = defect_detection[defect_detection[faults].sum(axis = 1) == 0] # Display the results if no_faults.empty: print(\"There are no observations with no faults.\") else: print(\"Number of observations with no faults:\") print(no_faults) De este an\u00e1lisis r\u00e1pido se puede concluir que no hay observaciones sin fallos. Ahora verifica si hay fallos que ocurren simult\u00e1neamente en una misma observaci\u00f3n. # Sum defect values per row defect_detection[faults].sum(axis = 1).value_counts() Se confirma que todas las observaciones contienen solo un tipo de fallo. Por lo tanto, este problema se mantiene como una clasificaci\u00f3n multicateg\u00f3rica en lugar de una clasificaci\u00f3n multietiqueta. Lo siguiente ser\u00e1 echar un vistazlo a la distribuci\u00f3n de todos los fallos para tener una visi\u00f3n clara de cu\u00e1ntas observaciones por fallo hay. sum_faults = defect_detection[faults].sum().sort_values(ascending=True) sum_faults La distribuci\u00f3n de tipos de fallos en el acero muestra que el fallo m\u00e1s com\u00fan es el de other faults , seguido por bumps y k_scratch . El fallo menos frecuente en el conjunto de datos es dirtiness (suciedad). El conjunto de datos presenta un claro desequilibrio, ya que el n\u00famero de instancias entre las diferentes clases de fallos var\u00eda ampliamente. Durante la etapa de modelado, ser\u00e1 necesario abordar este problema. Algunas t\u00e9cnicas utilizadas para manejar conjuntos de datos desequilibrados incluyen: re-muestreo, el uso de algoritmos robustos contra el desequilibrio de clases, o aplicar m\u00e9tricas que consideren la distribuci\u00f3n de clases, como el F1-score y la curva de precisi\u00f3n-recall. fault_types_distribution_graph = plt.figure(figsize = (10, 4)) ax = sns.barplot(sum_faults, color = \"#74add1\") for i in ax.containers: ax.bar_label(i,) plt.xlabel(\"Tipo de defecto\") plt.ylabel(\"Frecuencia\") plt.title(\"Distribuci\u00f3n de los tipos de defectos en las placas de acero\", size = 12) fault_types_distribution_pie = plt.figure(figsize = (11, 5)) colors = [\"#d73027\", \"#f46d43\", \"#fdae61\", \"#fee090\", \"#abd9e9\", \"#74add1\", \"#4575b4\"] plt.pie(sum_faults, labels = sum_faults.index, startangle = 90, autopct = \"%1.0f%%\", shadow = True, colors = colors) plt.axis(\"equal\") plt.legend() plt.title(\"Distribuci\u00f3n de los tipos de defectos en las placas de acero\", pad = 15) Features El siguiente paso es examinar cada caracter\u00edstica del conjunto de datos para obtener una visi\u00f3n general de sus distribuciones. Dado que todas est\u00e1n en diferentes escalas, ser\u00e1 necesario escalar las caracter\u00edsticas en los pr\u00f3ximos pasos. Los histogramas revelan informaci\u00f3n valiosa sobre la distribuci\u00f3n de las caracter\u00edsticas. Distribuci\u00f3n normal : Maximum_of_Luminosity, Empty_Index, Square_Index, Luminosity_Index, Minimum_of_Luminosity y Orientation_Index. Distribuci\u00f3n sesgada : Y_Minimum, Y_Maximum, Pixels_Areas, X_Perimeter, Y_Perimeter, Sum_of_Luminosity, Log_Y_Index y Edges_X_Index. Distribuci\u00f3n uniforme : X_Minimum, X_Maximum, EdgesIndex, Edges_Y_Index y SigmoidOfAreas. # Plot distribution of all features on the datasest fig, ax = plt.subplots(9, 3, figsize = (18, 32)) for i, col in enumerate(defect_detection.columns[:27]): sns.histplot(defect_detection[col], ax = ax[i//3][i%3], color = \"#74add1\") Matriz de correlaci\u00f3n correlations = defect_detection.corr() correlations correlation_heatmap_graph = plt.figure(figsize = (10, 8)) sns.heatmap(correlations, linewidths = 0.5, cmap = \"RdYlBu\") plt.title(\"Correlation Heatmap\", size = 12) La matriz de correlaci\u00f3n muestra las relaciones entre variables: el color rojo indica que la relaci\u00f3n es negativa, el azul que la relaci\u00f3n es positiva y el color amarillo que la relaci\u00f3n es baja o nula. Puesto que hay un alto n\u00famero de caracter\u00edsticas correlacionadas es necesario crear nuevas caracter\u00edsticas a partir de \u00e9stas para reducir el n\u00famero total de caracter\u00edsticas en el dataset y facilitar el modelado.","title":"Data Exploration"},{"location":"data-exploration/#exploratory-data-analysis","text":"En esta secci\u00f3n, se realiza un an\u00e1lisis exploratorio de datos en profundidad.","title":"Exploratory Data Analysis"},{"location":"data-exploration/#target-variable","text":"El primer paso es examinar la variable objetivo para obtener una visi\u00f3n general de su distribuci\u00f3n. El objetivo de este proyecto es predecir fallos en las placas de acero. Este conjunto de datos contiene 7 columnas que representan los 7 posibles fallos. Estas 7 columnas son la variable objetivo de este problema. # Plot distribution of all target columns on the datasest fig, ax = plt.subplots(2, 4, figsize = (20, 8)) ax = ax.flatten() for i, col in enumerate(defect_detection.columns[-7:]): sns.histplot(defect_detection[col], ax = ax[i], color = \"#74add1\") Verifica si hay observaciones sin fallos. # List of fault columns faults = [\"Pastry\", \"Z_Scratch\", \"K_Scratch\", \"Stains\", \"Dirtiness\", \"Bumps\", \"Other_Faults\"] # Get the observations with no faults no_faults = defect_detection[defect_detection[faults].sum(axis = 1) == 0] # Display the results if no_faults.empty: print(\"There are no observations with no faults.\") else: print(\"Number of observations with no faults:\") print(no_faults) De este an\u00e1lisis r\u00e1pido se puede concluir que no hay observaciones sin fallos. Ahora verifica si hay fallos que ocurren simult\u00e1neamente en una misma observaci\u00f3n. # Sum defect values per row defect_detection[faults].sum(axis = 1).value_counts() Se confirma que todas las observaciones contienen solo un tipo de fallo. Por lo tanto, este problema se mantiene como una clasificaci\u00f3n multicateg\u00f3rica en lugar de una clasificaci\u00f3n multietiqueta. Lo siguiente ser\u00e1 echar un vistazlo a la distribuci\u00f3n de todos los fallos para tener una visi\u00f3n clara de cu\u00e1ntas observaciones por fallo hay. sum_faults = defect_detection[faults].sum().sort_values(ascending=True) sum_faults La distribuci\u00f3n de tipos de fallos en el acero muestra que el fallo m\u00e1s com\u00fan es el de other faults , seguido por bumps y k_scratch . El fallo menos frecuente en el conjunto de datos es dirtiness (suciedad). El conjunto de datos presenta un claro desequilibrio, ya que el n\u00famero de instancias entre las diferentes clases de fallos var\u00eda ampliamente. Durante la etapa de modelado, ser\u00e1 necesario abordar este problema. Algunas t\u00e9cnicas utilizadas para manejar conjuntos de datos desequilibrados incluyen: re-muestreo, el uso de algoritmos robustos contra el desequilibrio de clases, o aplicar m\u00e9tricas que consideren la distribuci\u00f3n de clases, como el F1-score y la curva de precisi\u00f3n-recall. fault_types_distribution_graph = plt.figure(figsize = (10, 4)) ax = sns.barplot(sum_faults, color = \"#74add1\") for i in ax.containers: ax.bar_label(i,) plt.xlabel(\"Tipo de defecto\") plt.ylabel(\"Frecuencia\") plt.title(\"Distribuci\u00f3n de los tipos de defectos en las placas de acero\", size = 12) fault_types_distribution_pie = plt.figure(figsize = (11, 5)) colors = [\"#d73027\", \"#f46d43\", \"#fdae61\", \"#fee090\", \"#abd9e9\", \"#74add1\", \"#4575b4\"] plt.pie(sum_faults, labels = sum_faults.index, startangle = 90, autopct = \"%1.0f%%\", shadow = True, colors = colors) plt.axis(\"equal\") plt.legend() plt.title(\"Distribuci\u00f3n de los tipos de defectos en las placas de acero\", pad = 15)","title":"Target Variable"},{"location":"data-exploration/#features","text":"El siguiente paso es examinar cada caracter\u00edstica del conjunto de datos para obtener una visi\u00f3n general de sus distribuciones. Dado que todas est\u00e1n en diferentes escalas, ser\u00e1 necesario escalar las caracter\u00edsticas en los pr\u00f3ximos pasos. Los histogramas revelan informaci\u00f3n valiosa sobre la distribuci\u00f3n de las caracter\u00edsticas. Distribuci\u00f3n normal : Maximum_of_Luminosity, Empty_Index, Square_Index, Luminosity_Index, Minimum_of_Luminosity y Orientation_Index. Distribuci\u00f3n sesgada : Y_Minimum, Y_Maximum, Pixels_Areas, X_Perimeter, Y_Perimeter, Sum_of_Luminosity, Log_Y_Index y Edges_X_Index. Distribuci\u00f3n uniforme : X_Minimum, X_Maximum, EdgesIndex, Edges_Y_Index y SigmoidOfAreas. # Plot distribution of all features on the datasest fig, ax = plt.subplots(9, 3, figsize = (18, 32)) for i, col in enumerate(defect_detection.columns[:27]): sns.histplot(defect_detection[col], ax = ax[i//3][i%3], color = \"#74add1\")","title":"Features"},{"location":"data-exploration/#matriz-de-correlacion","text":"correlations = defect_detection.corr() correlations correlation_heatmap_graph = plt.figure(figsize = (10, 8)) sns.heatmap(correlations, linewidths = 0.5, cmap = \"RdYlBu\") plt.title(\"Correlation Heatmap\", size = 12) La matriz de correlaci\u00f3n muestra las relaciones entre variables: el color rojo indica que la relaci\u00f3n es negativa, el azul que la relaci\u00f3n es positiva y el color amarillo que la relaci\u00f3n es baja o nula. Puesto que hay un alto n\u00famero de caracter\u00edsticas correlacionadas es necesario crear nuevas caracter\u00edsticas a partir de \u00e9stas para reducir el n\u00famero total de caracter\u00edsticas en el dataset y facilitar el modelado.","title":"Matriz de correlaci\u00f3n"},{"location":"feature-engineering/","text":"Feature Engineering De acuerdo con los resultados obtenidos de la matrix de correlaci\u00f3n, se crean nuevas variables. Las variables que ya no sean necesarias se eliminar\u00e1n del conjunto de datos. # Create new variables defect_detection[\"X_Range\"] = defect_detection[\"X_Maximum\"] - defect_detection[\"X_Minimum\"] defect_detection[\"Y_Range\"] = defect_detection[\"Y_Maximum\"] - defect_detection[\"Y_Minimum\"] defect_detection[\"Defect_Area\"] = (defect_detection[\"X_Perimeter\"] * defect_detection[\"Y_Perimeter\"]) defect_detection[\"Luminosity_Range\"] = defect_detection[\"Maximum_of_Luminosity\"] - defect_detection[\"Minimum_of_Luminosity\"] defect_detection[\"Edge\"] = defect_detection[\"Edges_Index\"] / (defect_detection[\"Edges_X_Index\"] * defect_detection[\"Edges_Y_Index\"]) defect_detection[\"Outside_X_Range\"] = defect_detection[\"Outside_X_Index\"] * defect_detection[\"X_Range\"] defect_detection[\"Log_Area\"] = defect_detection[\"Log_of_Areas\"] / (0.000001 + defect_detection[\"Log_X_Index\"] * defect_detection [\"Log_Y_Index\"]) defect_detection[\"Luminosity_Sum_Range\"] = defect_detection[\"Sum_of_Luminosity\"] * defect_detection[\"Luminosity_Range\"] defect_detection[\"Log_Area_Sigmoid\"] = defect_detection[\"Log_Area\"] * defect_detection[\"Sigmoid_of_Areas\"] defect_detection.columns # Drop unnecessary columns columns_to_drop = [\"X_Minimum\", \"X_Maximum\", \"Y_Minimum\", \"Y_Maximum\", \"X_Perimeter\", \"Y_Perimeter\", \"Minimum_of_Luminosity\", \"Maximum_of_Luminosity\", \"Outside_X_Index\", \"Edges_Index\", \"Edges_X_Index\", \"Edges_Y_Index\",\"Log_of_Areas\", \"Log_X_Index\", \"Log_Y_Index\", \"Sum_of_Luminosity\", \"Luminosity_Range\", \"Sigmoid_of_Areas\"] defect_detection = defect_detection.drop(columns_to_drop, axis = 1) defect_detection # Show columns after dropping the unnecessary ones defect_detection.columns # Plot again the correlation matrix with the new features correlations_2 = defect_detection.corr() correlations_2 correlation_heatmap_graph_2 = plt.figure(figsize = (10, 8)) sns.heatmap(correlations_2, linewidths = 0.5, cmap = \"RdYlBu\") plt.title(\"Correlation Heatmap\", size = 12) # Save the feature engineered dataset defect_detection.to_csv(\"../data/feature-engineering/Steel-plates-faults-feature-engineering-dataset.csv\")","title":"Feature Engineering"},{"location":"feature-engineering/#feature-engineering","text":"De acuerdo con los resultados obtenidos de la matrix de correlaci\u00f3n, se crean nuevas variables. Las variables que ya no sean necesarias se eliminar\u00e1n del conjunto de datos. # Create new variables defect_detection[\"X_Range\"] = defect_detection[\"X_Maximum\"] - defect_detection[\"X_Minimum\"] defect_detection[\"Y_Range\"] = defect_detection[\"Y_Maximum\"] - defect_detection[\"Y_Minimum\"] defect_detection[\"Defect_Area\"] = (defect_detection[\"X_Perimeter\"] * defect_detection[\"Y_Perimeter\"]) defect_detection[\"Luminosity_Range\"] = defect_detection[\"Maximum_of_Luminosity\"] - defect_detection[\"Minimum_of_Luminosity\"] defect_detection[\"Edge\"] = defect_detection[\"Edges_Index\"] / (defect_detection[\"Edges_X_Index\"] * defect_detection[\"Edges_Y_Index\"]) defect_detection[\"Outside_X_Range\"] = defect_detection[\"Outside_X_Index\"] * defect_detection[\"X_Range\"] defect_detection[\"Log_Area\"] = defect_detection[\"Log_of_Areas\"] / (0.000001 + defect_detection[\"Log_X_Index\"] * defect_detection [\"Log_Y_Index\"]) defect_detection[\"Luminosity_Sum_Range\"] = defect_detection[\"Sum_of_Luminosity\"] * defect_detection[\"Luminosity_Range\"] defect_detection[\"Log_Area_Sigmoid\"] = defect_detection[\"Log_Area\"] * defect_detection[\"Sigmoid_of_Areas\"] defect_detection.columns # Drop unnecessary columns columns_to_drop = [\"X_Minimum\", \"X_Maximum\", \"Y_Minimum\", \"Y_Maximum\", \"X_Perimeter\", \"Y_Perimeter\", \"Minimum_of_Luminosity\", \"Maximum_of_Luminosity\", \"Outside_X_Index\", \"Edges_Index\", \"Edges_X_Index\", \"Edges_Y_Index\",\"Log_of_Areas\", \"Log_X_Index\", \"Log_Y_Index\", \"Sum_of_Luminosity\", \"Luminosity_Range\", \"Sigmoid_of_Areas\"] defect_detection = defect_detection.drop(columns_to_drop, axis = 1) defect_detection # Show columns after dropping the unnecessary ones defect_detection.columns # Plot again the correlation matrix with the new features correlations_2 = defect_detection.corr() correlations_2 correlation_heatmap_graph_2 = plt.figure(figsize = (10, 8)) sns.heatmap(correlations_2, linewidths = 0.5, cmap = \"RdYlBu\") plt.title(\"Correlation Heatmap\", size = 12) # Save the feature engineered dataset defect_detection.to_csv(\"../data/feature-engineering/Steel-plates-faults-feature-engineering-dataset.csv\")","title":"Feature Engineering"},{"location":"modeling/","text":"Modelado El objetivo de este ejercicio es construir un modelo que clasifique los defectos de placas de acero industrial. Para ello, el primer paso es encontrar el algoritmo que mejor rendimiento presente y a partir de ah\u00ed, se afinar\u00e1n los hiperpar\u00e1metros del este modelo para encontrar la mejor versi\u00f3n del modelo ganador. Los algoritmos elegidos para comenzar el modelado son: Decision Tree , Random Forest , XGBoost , Support Vector Machine y Multilayer Perceptron . # 1. Decision Tree print(\"Decision Tree Model:\") dt_model = DecisionTreeClassifier(random_state = 42) dt_model.fit(X_train_scaled, y_train) dt_predictions = dt_model.predict(X_test_scaled) print(classification_report(y_test, dt_predictions)) dt_cm = confusion_matrix(y_test, dt_predictions) ConfusionMatrixDisplay(confusion_matrix = dt_cm).plot(cmap = \"PuBu\") plt.title(\"Matriz de Confusi\u00f3n del modelo Decision Tree\") plt.xlabel(\"Predicci\u00f3n\") plt.ylabel(\"Valor real\") # 2. Random Forest print(\"Random Forest Model:\") rf_model = RandomForestClassifier(random_state = 42) rf_model.fit(X_train_scaled, y_train) rf_predictions = rf_model.predict(X_test_scaled) print(classification_report(y_test, rf_predictions)) rf_cm = confusion_matrix(y_test, rf_predictions) ConfusionMatrixDisplay(confusion_matrix = rf_cm).plot(cmap = \"PuBu\") plt.title(\"Matriz de Confusi\u00f3n del modelo Random Forest\") plt.xlabel(\"Predicci\u00f3n\") plt.ylabel(\"Valor real\") # 3. XGBoost print(\"XGBoost Model:\") xgb_model = XGBClassifier(random_state = 42) xgb_model.fit(X_train_scaled, y_train) xgb_predictions = xgb_model.predict(X_test_scaled) print(classification_report(y_test, xgb_predictions)) xgb_cm = confusion_matrix(y_test, xgb_predictions) ConfusionMatrixDisplay(confusion_matrix = xgb_cm).plot(cmap = \"PuBu\") plt.title(\"Matriz de Confusi\u00f3n del modelo XGBoost\") plt.xlabel(\"Predicci\u00f3n\") plt.ylabel(\"Valor real\") # 4. Support Vector Machine print(\"Support Vector Machine Model:\") svm_model = SVC(kernel='linear', random_state=42) svm_model.fit(X_train_scaled, y_train) svm_predictions = svm_model.predict(X_test_scaled) print(classification_report(y_test, svm_predictions)) svm_cm = confusion_matrix(y_test, svm_predictions) ConfusionMatrixDisplay(confusion_matrix = svm_cm).plot(cmap = \"PuBu\") plt.title(\"Matriz de Confusi\u00f3n del modelo Support Vector Machine\") plt.xlabel(\"Predicci\u00f3n\") plt.ylabel(\"Valor real\") # 5. Multilayer Perceptron print(\"Multilayer Perceptron Model:\") mlp_model = MLPClassifier(random_state = 42, max_iter = 500) mlp_model.fit(X_train_scaled, y_train) mlp_predictions = mlp_model.predict(X_test_scaled) print(classification_report(y_test, mlp_predictions)) mlp_cm = confusion_matrix(y_test, mlp_predictions) ConfusionMatrixDisplay(confusion_matrix = mlp_cm).plot(cmap = \"PuBu\") plt.title(\"Matriz de Confusi\u00f3n del modelo Multilayer Perceptron\") plt.xlabel(\"Predicci\u00f3n\") plt.ylabel(\"Valor real\") Entre todos los modelos probados, XGBoost destaca como el m\u00e1s efectivo, alcanzando una precisi\u00f3n de 0.79 y sobresaliendo tanto en precisi\u00f3n como en recall en varias clases. Su F1-score macro promedio de 0.78 refuerza su rendimiento equilibrado. Random Forest y Multilayer Perceptron le siguen de cerca con una precisi\u00f3n de 0.76 . Ambos ofrecen resultados s\u00f3lidos, pero no logran igualar la efectividad de XGBoost. Por otro lado, \u00c1rbol de Decisi\u00f3n y M\u00e1quina de Vectores de Soporte solo alcanzan una precisi\u00f3n de 0.70 , teniendo m\u00e1s dificultades en la clasificaci\u00f3n de algunas clases. XGBoost es la opci\u00f3n m\u00e1s acertada si el objetivo es maximizar la precisi\u00f3n del model y tener un rendimiento equilibrado. Adem\u00e1s, un ajuste adicional de los hiperpar\u00e1metros podr\u00eda llevar su rendimiento a otro nivel.","title":"Modeling"},{"location":"modeling/#modelado","text":"El objetivo de este ejercicio es construir un modelo que clasifique los defectos de placas de acero industrial. Para ello, el primer paso es encontrar el algoritmo que mejor rendimiento presente y a partir de ah\u00ed, se afinar\u00e1n los hiperpar\u00e1metros del este modelo para encontrar la mejor versi\u00f3n del modelo ganador. Los algoritmos elegidos para comenzar el modelado son: Decision Tree , Random Forest , XGBoost , Support Vector Machine y Multilayer Perceptron . # 1. Decision Tree print(\"Decision Tree Model:\") dt_model = DecisionTreeClassifier(random_state = 42) dt_model.fit(X_train_scaled, y_train) dt_predictions = dt_model.predict(X_test_scaled) print(classification_report(y_test, dt_predictions)) dt_cm = confusion_matrix(y_test, dt_predictions) ConfusionMatrixDisplay(confusion_matrix = dt_cm).plot(cmap = \"PuBu\") plt.title(\"Matriz de Confusi\u00f3n del modelo Decision Tree\") plt.xlabel(\"Predicci\u00f3n\") plt.ylabel(\"Valor real\") # 2. Random Forest print(\"Random Forest Model:\") rf_model = RandomForestClassifier(random_state = 42) rf_model.fit(X_train_scaled, y_train) rf_predictions = rf_model.predict(X_test_scaled) print(classification_report(y_test, rf_predictions)) rf_cm = confusion_matrix(y_test, rf_predictions) ConfusionMatrixDisplay(confusion_matrix = rf_cm).plot(cmap = \"PuBu\") plt.title(\"Matriz de Confusi\u00f3n del modelo Random Forest\") plt.xlabel(\"Predicci\u00f3n\") plt.ylabel(\"Valor real\") # 3. XGBoost print(\"XGBoost Model:\") xgb_model = XGBClassifier(random_state = 42) xgb_model.fit(X_train_scaled, y_train) xgb_predictions = xgb_model.predict(X_test_scaled) print(classification_report(y_test, xgb_predictions)) xgb_cm = confusion_matrix(y_test, xgb_predictions) ConfusionMatrixDisplay(confusion_matrix = xgb_cm).plot(cmap = \"PuBu\") plt.title(\"Matriz de Confusi\u00f3n del modelo XGBoost\") plt.xlabel(\"Predicci\u00f3n\") plt.ylabel(\"Valor real\") # 4. Support Vector Machine print(\"Support Vector Machine Model:\") svm_model = SVC(kernel='linear', random_state=42) svm_model.fit(X_train_scaled, y_train) svm_predictions = svm_model.predict(X_test_scaled) print(classification_report(y_test, svm_predictions)) svm_cm = confusion_matrix(y_test, svm_predictions) ConfusionMatrixDisplay(confusion_matrix = svm_cm).plot(cmap = \"PuBu\") plt.title(\"Matriz de Confusi\u00f3n del modelo Support Vector Machine\") plt.xlabel(\"Predicci\u00f3n\") plt.ylabel(\"Valor real\") # 5. Multilayer Perceptron print(\"Multilayer Perceptron Model:\") mlp_model = MLPClassifier(random_state = 42, max_iter = 500) mlp_model.fit(X_train_scaled, y_train) mlp_predictions = mlp_model.predict(X_test_scaled) print(classification_report(y_test, mlp_predictions)) mlp_cm = confusion_matrix(y_test, mlp_predictions) ConfusionMatrixDisplay(confusion_matrix = mlp_cm).plot(cmap = \"PuBu\") plt.title(\"Matriz de Confusi\u00f3n del modelo Multilayer Perceptron\") plt.xlabel(\"Predicci\u00f3n\") plt.ylabel(\"Valor real\") Entre todos los modelos probados, XGBoost destaca como el m\u00e1s efectivo, alcanzando una precisi\u00f3n de 0.79 y sobresaliendo tanto en precisi\u00f3n como en recall en varias clases. Su F1-score macro promedio de 0.78 refuerza su rendimiento equilibrado. Random Forest y Multilayer Perceptron le siguen de cerca con una precisi\u00f3n de 0.76 . Ambos ofrecen resultados s\u00f3lidos, pero no logran igualar la efectividad de XGBoost. Por otro lado, \u00c1rbol de Decisi\u00f3n y M\u00e1quina de Vectores de Soporte solo alcanzan una precisi\u00f3n de 0.70 , teniendo m\u00e1s dificultades en la clasificaci\u00f3n de algunas clases. XGBoost es la opci\u00f3n m\u00e1s acertada si el objetivo es maximizar la precisi\u00f3n del model y tener un rendimiento equilibrado. Adem\u00e1s, un ajuste adicional de los hiperpar\u00e1metros podr\u00eda llevar su rendimiento a otro nivel.","title":"Modelado"},{"location":"pre-processing/","text":"Pre-processing Antes de comenzar con el modelado, ser\u00e1 necesario hacer unas transformaciones en el conjunto de datos. Nueva columna de Fallos Crea una nueva columna que contenga todos los tipos de fallos. Esta columna ser\u00e1 la variable objetivo y facilitar\u00e1 el modelado y la predicci\u00f3n. # List of faults and their corresponding values fault_mapping = {\"Pastry\": 0, \"Z_Scratch\": 1, \"K_Scratch\": 2, \"Stains\": 3, \"Dirtiness\": 4, \"Bumps\": 5, \"Other_Faults\": 6} # Initialize the Faults column defect_detection[\"Faults\"] = 0 # Loop through each fault and assign the corresponding value for fault, value in fault_mapping.items(): defect_detection.loc[defect_detection[fault] == 1, \"Faults\"] = value # Display the first few rows defect_detection.head() # Drop individual fault columns defect_detection.drop(faults, axis = 1, inplace=True) defect_detection.head() Dividir los datos Al dividir los datos en un conjunto de entrenamiento y uno de prueba, se asegura que el escalado se base \u00fanicamente en los datos de entrenamiento, evitando que cualquier informaci\u00f3n del conjunto de prueba se filtren en el modelo durante el entrenamiento. # Independent features X = defect_detection.drop(\"Faults\", axis = 1) # Dependent or target variable y = defect_detection[\"Faults\"] # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42) print(\"Training set - X_train shape:\", X_train.shape) print(\"Testing set - X_test shape:\", X_test.shape) print(\"Training set - y_train shape:\", y_train.shape) print(\"Testing set - y_test shape:\", y_test.shape) Escalado de caracter\u00edsticas El m\u00e9todo utilizado es el de escalado est\u00e1ndar. # Initialize the scaler scaler = StandardScaler() # Fit the scaler on the training data and transform it X_train_scaled = scaler.fit_transform(X_train) # Transform the test data using the same scaler X_test_scaled = scaler.transform(X_test)","title":"Pre-processing"},{"location":"pre-processing/#pre-processing","text":"Antes de comenzar con el modelado, ser\u00e1 necesario hacer unas transformaciones en el conjunto de datos. Nueva columna de Fallos Crea una nueva columna que contenga todos los tipos de fallos. Esta columna ser\u00e1 la variable objetivo y facilitar\u00e1 el modelado y la predicci\u00f3n. # List of faults and their corresponding values fault_mapping = {\"Pastry\": 0, \"Z_Scratch\": 1, \"K_Scratch\": 2, \"Stains\": 3, \"Dirtiness\": 4, \"Bumps\": 5, \"Other_Faults\": 6} # Initialize the Faults column defect_detection[\"Faults\"] = 0 # Loop through each fault and assign the corresponding value for fault, value in fault_mapping.items(): defect_detection.loc[defect_detection[fault] == 1, \"Faults\"] = value # Display the first few rows defect_detection.head() # Drop individual fault columns defect_detection.drop(faults, axis = 1, inplace=True) defect_detection.head() Dividir los datos Al dividir los datos en un conjunto de entrenamiento y uno de prueba, se asegura que el escalado se base \u00fanicamente en los datos de entrenamiento, evitando que cualquier informaci\u00f3n del conjunto de prueba se filtren en el modelo durante el entrenamiento. # Independent features X = defect_detection.drop(\"Faults\", axis = 1) # Dependent or target variable y = defect_detection[\"Faults\"] # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42) print(\"Training set - X_train shape:\", X_train.shape) print(\"Testing set - X_test shape:\", X_test.shape) print(\"Training set - y_train shape:\", y_train.shape) print(\"Testing set - y_test shape:\", y_test.shape) Escalado de caracter\u00edsticas El m\u00e9todo utilizado es el de escalado est\u00e1ndar. # Initialize the scaler scaler = StandardScaler() # Fit the scaler on the training data and transform it X_train_scaled = scaler.fit_transform(X_train) # Transform the test data using the same scaler X_test_scaled = scaler.transform(X_test)","title":"Pre-processing"},{"location":"results/","text":"Conclusiones El objetivo de este ejercicio fue construir un modelo de machine learning para detectar anomal\u00edas o defectos en placas de acero industrial. Para lograrlo, se inici\u00f3 con la limpieza del conjunto de datos, un paso fundamental para asegurar la integridad y confiabilidad de la informaci\u00f3n. Seguidamente, se realiz\u00f3 un an\u00e1lisis exploratorio detallado que permiti\u00f3 comprender mejor la distribuci\u00f3n y las relaciones entre las variables. Este an\u00e1lisis incluy\u00f3 la evaluaci\u00f3n individual de cada variable y la exploraci\u00f3n de interacciones significativas entre pares de ellas. Esto permiti\u00f3 crear nuevas variables que hacen m\u00e1s f\u00e1cil el modelado. Para determinar el modelo \u00f3ptimo para la predicci\u00f3n de anomal\u00edas, se probaron varios enfoques: decision tree, random forest, xgboost, support vector machine y multilayer perceptron. Se concluy\u00f3 que el modelo con mejor rendimiento fue XGBoost. Posteriormente, se busc\u00f3 encontrar los hiperpar\u00e1metros ideales para este modelo. Dado que durante la exploraci\u00f3n de datos se identific\u00f3 un desbalance en el conjunto, se aplic\u00f3 la t\u00e9cnica de re-muestreo SMOTE . Este modelo no solo mostr\u00f3 el mejor rendimiento, sino que tambi\u00e9n destac\u00f3 en precisi\u00f3n y recall. En resumen, se construy\u00f3 un modelo de machine learning robusto capaz de distinguir entre clases minoritarias y mayoritarias, ofreciendo un rendimiento s\u00f3lido y constituyendo la mejor opci\u00f3n para resolver este problema..","title":"Results"},{"location":"results/#conclusiones","text":"El objetivo de este ejercicio fue construir un modelo de machine learning para detectar anomal\u00edas o defectos en placas de acero industrial. Para lograrlo, se inici\u00f3 con la limpieza del conjunto de datos, un paso fundamental para asegurar la integridad y confiabilidad de la informaci\u00f3n. Seguidamente, se realiz\u00f3 un an\u00e1lisis exploratorio detallado que permiti\u00f3 comprender mejor la distribuci\u00f3n y las relaciones entre las variables. Este an\u00e1lisis incluy\u00f3 la evaluaci\u00f3n individual de cada variable y la exploraci\u00f3n de interacciones significativas entre pares de ellas. Esto permiti\u00f3 crear nuevas variables que hacen m\u00e1s f\u00e1cil el modelado. Para determinar el modelo \u00f3ptimo para la predicci\u00f3n de anomal\u00edas, se probaron varios enfoques: decision tree, random forest, xgboost, support vector machine y multilayer perceptron. Se concluy\u00f3 que el modelo con mejor rendimiento fue XGBoost. Posteriormente, se busc\u00f3 encontrar los hiperpar\u00e1metros ideales para este modelo. Dado que durante la exploraci\u00f3n de datos se identific\u00f3 un desbalance en el conjunto, se aplic\u00f3 la t\u00e9cnica de re-muestreo SMOTE . Este modelo no solo mostr\u00f3 el mejor rendimiento, sino que tambi\u00e9n destac\u00f3 en precisi\u00f3n y recall. En resumen, se construy\u00f3 un modelo de machine learning robusto capaz de distinguir entre clases minoritarias y mayoritarias, ofreciendo un rendimiento s\u00f3lido y constituyendo la mejor opci\u00f3n para resolver este problema..","title":"Conclusiones"},{"location":"xgboost/","text":"XGBoost XGBoost es el modelo seleccionado para abordar esta tarea de clasificaci\u00f3n. Ahora es el momento de comprender el modelo y encontrar los mejores hiperpar\u00e1metros. Hyperparameter Tuning Primero, se optimiza el modelo para mejorar su rendimiento. Este modelo ajustado funciona mejor que el original, pero a\u00fan presenta dificultades para clasificar las clases minoritarias, como la clase 4, que corresponde al defecto \"suciedad\" y tiene muy pocas observaciones en el conjunto de datos. Para solucionar este problema, es necesario aplicar una t\u00e9cnica de muestreo. Dado que las observaciones son escasas, decid\u00ed utilizar la t\u00e9cnica SMOTE para mejorar la clasificaci\u00f3n. # Define the model model = xgb.XGBClassifier(random_state = 42) # Define the hyperparameter grid param_grid = {\"max_depth\": [3, 5, 7], \"learning_rate\": [0.01, 0.1, 0.2], \"n_estimators\": [100, 200, 300], \"subsample\": [0.6, 0.8, 1.0]} # Set up Grid Search grid_search = GridSearchCV(estimator = model, param_grid = param_grid, scoring = \"f1_macro\", cv = 3, verbose = 1, n_jobs = -1) # Fit the model grid_search.fit(X_train_scaled, y_train) # Print the best parameters and score print(\"Best Parameters:\", grid_search.best_params_) print(\"Best Score:\", grid_search.best_score_) # Evaluate on the test set best_model = grid_search.best_estimator_ y_pred = best_model.predict(X_test_scaled) print(classification_report(y_test, y_pred)) SMOTE SMOTE es una t\u00e9cnica de re-muestreo que crea nuevas muestras ajustando ligeramente los datos existentes hacia sus vecinos. Funciona eligiendo aleatoriamente una muestra de la clase minoritaria, identificando sus k vecinos m\u00e1s cercanos y generando nuevos puntos de datos al escalar la distancia hacia esos vecinos. De esta manera, se mantiene la integridad de la clase minoritaria y se enriquece el conjunto de datos. Una vez dividido el conjunto de datos en entrenamiento y prueba, aplicamos SMOTE solamente al conjunto de entrenamiento. Esto asegura que el conjunto de prueba siga siendo una representaci\u00f3n fiel de la distribuci\u00f3n original de los datos y previene cualquier filtraci\u00f3n de informaci\u00f3n desde el conjunto de entrenamiento. As\u00ed, garantizamos que el proceso de evaluaci\u00f3n sea m\u00e1s s\u00f3lido y confiable. # Apply SMOTE to the training data smote = SMOTE(random_state = 42) X_resampled, y_resampled = smote.fit_resample(X_train_scaled, y_train) # Fit the XGBoost model on the resampled data xgb_smote_model = XGBClassifier(random_state = 42) xgb_smote_model.fit(X_resampled, y_resampled) # Evaluate the model on the test set xgb_smote_pred = xgb_smote_model.predict(X_test_scaled) print(classification_report(y_test, xgb_smote_pred)) El modelo XGBoost con SMOTE se destaca claramente como el mejor en comparaci\u00f3n con el modelo XGBoost ajustado, logrando una precisi\u00f3n del 80% y un F1-score macro promedio de 0.80. Aunque la precisi\u00f3n de ambos modelos es similar, uno de los puntos fuertes de este modelo es la capacidad para identificar eficazmente instancias de clases minoritarias, como la clase 4, donde logr\u00f3 un valor de recall del 88%, mientras que el modelo original alcanz\u00f3 62% y el ajustado solo alcanz\u00f3 el 50%. Adem\u00e1s, el modelo SMOTE muestra un rendimiento constante en las clases mayoritarias, lo que demuestra su fiabilidad y un rendimiento bastante s\u00f3lido en general. Esto lo convierte en la mejor opci\u00f3n para aplicaciones del mundo real, especialmente en conjuntos de datos desbalanceados donde cada clase es importante. # Save the trained model as pickle format with open(\"../model/xgboost_model.pkl\", \"wb\") as file: pickle.dump(xgb_model, file) # Save the trained model as a json file xgb_model.save_model(\"../model/xgboost_model.json\") Matriz de Confusi\u00f3n Analizar la matriz de confusi\u00f3n nos ayuda a comprender las clasificaciones incorrectas y a identificar \u00e1reas potenciales de mejora. # Plot XGBoost Confusion Matrix xgb_smote_model_cm = confusion_matrix(y_test, xgb_predictions) ConfusionMatrixDisplay(confusion_matrix = xgb_smote_model_cm).plot(cmap = \"PuBu\") plt.title(\"Matriz de confusi\u00f3n del modelo XGBoost\") plt.xlabel(\"Predicci\u00f3n\") plt.ylabel(\"Valor real\") Importancia de las caracter\u00edsticas Identificar las caracter\u00edsticas clave que impulsan las predicciones es fundamental para comprender c\u00f3mo el modelo toma decisiones. # Plot feature importance xgboost_feature_importance = plt.figure(figsize = (15, 3)) xgb.plot_importance(xgb_smote_model, importance_type = \"weight\", color = \"#74add1\", title = \"Importancia de caracter\u00edsticas de XGBoost\", xlabel = \"Peso\", ylabel = \"Caracter\u00edsticas\") ROC AUC Eval\u00faa el rendimiento del modelo de manera integral. # Binarize the output y_bin = label_binarize(y_test, classes = [0, 1, 2, 3, 4, 5, 6]) y_score = xgb_smote_model.predict_proba(X_test_scaled) # Compute ROC AUC for each class roc_auc = roc_auc_score(y_bin, y_score, average = \"macro\") print(f\"XGBoost Model ROC AUC Score: {roc_auc:.2f}\") # Binarize the output y_bin = label_binarize(y_test, classes = [0, 1, 2, 3, 4, 5, 6]) n_classes = y_bin.shape[1] # Compute ROC curve and ROC area for each class fpr = dict() tpr = dict() roc_auc = dict() for i in range(n_classes): fpr[i], tpr[i], _ = roc_curve(y_bin[:, i], xgb_smote_model.predict_proba(X_test_scaled)[:, i]) roc_auc[i] = auc(fpr[i], tpr[i]) # Plot the curves plt.figure(figsize = (11, 4)) for i in range(n_classes): plt.plot(fpr[i], tpr[i], color = colors[i], label = \"ROC curve of class {0} (area = {1:0.2f})\".format(i, roc_auc[i])) plt.plot([0, 1], [0, 1], \"k--\") plt.xlim([0.0, 1.0]) plt.ylim([0.0, 1.05]) plt.xlabel(\"Tasa de Falsos Positivos\") plt.ylabel(\"Tasa de Verdaderos Positivos\") plt.title(\"Curva Caracter\u00edstica de Operaci\u00f3n del Receptor (ROC)\") plt.legend(loc = \"lower right\") Curvas de Aprendizaje Eval\u00faa el comportamiento del entrenamiento del modelo y detecta problemas de sesgo. # Plot the learning curves of the model plt.figure(figsize = (11, 4)) train_sizes, train_scores, test_scores = learning_curve(xgb_smote_model, X_resampled, y_resampled, cv = 5, n_jobs = -1, train_sizes = np.linspace(0.1, 1.0, 10)) train_scores_mean = train_scores.mean(axis = 1) test_scores_mean = test_scores.mean(axis = 1) plt.plot(train_sizes, train_scores_mean, color = \"#74add1\", label = \"Puntuaci\u00f3n de entrenamiento\") plt.plot(train_sizes, test_scores_mean, color = \"#f46d43\", label = \"Puntuaci\u00f3n de validaci\u00f3n cruzada\") plt.title(\"Curva de aprendizaje del modelo XGBoost\") plt.xlabel(\"Tama\u00f1o de entrenamiento\") plt.ylabel(\"Puntuaci\u00f3n\") plt.legend(loc = \"best\") Curva de Precisi\u00f3n-Recall Eval\u00faa el rendimiento del modelo en t\u00e9rminos de precisi\u00f3n y recuperaci\u00f3n, especialmente \u00fatil en casos de clases desbalanceadas. # Plot the precision-recall curve plt.figure(figsize=(11, 4)) for i in range(n_classes): precision, recall, _ = precision_recall_curve(y_bin[:, i], xgb_smote_model.predict_proba(X_test_scaled)[:, i]) plt.plot(recall, precision, color = colors[i], label = f\"Curva de Precisi\u00f3n-Recuperaci\u00f3n de la clase {i}\") plt.xlabel(\"Recuperaci\u00f3n\") plt.ylabel(\"Precisi\u00f3n\") plt.title(\"Curva de Precisi\u00f3n-Recuperaci\u00f3n\") plt.legend(loc = \"lower left\")","title":"XGBoost"},{"location":"xgboost/#xgboost","text":"XGBoost es el modelo seleccionado para abordar esta tarea de clasificaci\u00f3n. Ahora es el momento de comprender el modelo y encontrar los mejores hiperpar\u00e1metros.","title":"XGBoost"},{"location":"xgboost/#hyperparameter-tuning","text":"Primero, se optimiza el modelo para mejorar su rendimiento. Este modelo ajustado funciona mejor que el original, pero a\u00fan presenta dificultades para clasificar las clases minoritarias, como la clase 4, que corresponde al defecto \"suciedad\" y tiene muy pocas observaciones en el conjunto de datos. Para solucionar este problema, es necesario aplicar una t\u00e9cnica de muestreo. Dado que las observaciones son escasas, decid\u00ed utilizar la t\u00e9cnica SMOTE para mejorar la clasificaci\u00f3n. # Define the model model = xgb.XGBClassifier(random_state = 42) # Define the hyperparameter grid param_grid = {\"max_depth\": [3, 5, 7], \"learning_rate\": [0.01, 0.1, 0.2], \"n_estimators\": [100, 200, 300], \"subsample\": [0.6, 0.8, 1.0]} # Set up Grid Search grid_search = GridSearchCV(estimator = model, param_grid = param_grid, scoring = \"f1_macro\", cv = 3, verbose = 1, n_jobs = -1) # Fit the model grid_search.fit(X_train_scaled, y_train) # Print the best parameters and score print(\"Best Parameters:\", grid_search.best_params_) print(\"Best Score:\", grid_search.best_score_) # Evaluate on the test set best_model = grid_search.best_estimator_ y_pred = best_model.predict(X_test_scaled) print(classification_report(y_test, y_pred))","title":"Hyperparameter Tuning"},{"location":"xgboost/#smote","text":"SMOTE es una t\u00e9cnica de re-muestreo que crea nuevas muestras ajustando ligeramente los datos existentes hacia sus vecinos. Funciona eligiendo aleatoriamente una muestra de la clase minoritaria, identificando sus k vecinos m\u00e1s cercanos y generando nuevos puntos de datos al escalar la distancia hacia esos vecinos. De esta manera, se mantiene la integridad de la clase minoritaria y se enriquece el conjunto de datos. Una vez dividido el conjunto de datos en entrenamiento y prueba, aplicamos SMOTE solamente al conjunto de entrenamiento. Esto asegura que el conjunto de prueba siga siendo una representaci\u00f3n fiel de la distribuci\u00f3n original de los datos y previene cualquier filtraci\u00f3n de informaci\u00f3n desde el conjunto de entrenamiento. As\u00ed, garantizamos que el proceso de evaluaci\u00f3n sea m\u00e1s s\u00f3lido y confiable. # Apply SMOTE to the training data smote = SMOTE(random_state = 42) X_resampled, y_resampled = smote.fit_resample(X_train_scaled, y_train) # Fit the XGBoost model on the resampled data xgb_smote_model = XGBClassifier(random_state = 42) xgb_smote_model.fit(X_resampled, y_resampled) # Evaluate the model on the test set xgb_smote_pred = xgb_smote_model.predict(X_test_scaled) print(classification_report(y_test, xgb_smote_pred)) El modelo XGBoost con SMOTE se destaca claramente como el mejor en comparaci\u00f3n con el modelo XGBoost ajustado, logrando una precisi\u00f3n del 80% y un F1-score macro promedio de 0.80. Aunque la precisi\u00f3n de ambos modelos es similar, uno de los puntos fuertes de este modelo es la capacidad para identificar eficazmente instancias de clases minoritarias, como la clase 4, donde logr\u00f3 un valor de recall del 88%, mientras que el modelo original alcanz\u00f3 62% y el ajustado solo alcanz\u00f3 el 50%. Adem\u00e1s, el modelo SMOTE muestra un rendimiento constante en las clases mayoritarias, lo que demuestra su fiabilidad y un rendimiento bastante s\u00f3lido en general. Esto lo convierte en la mejor opci\u00f3n para aplicaciones del mundo real, especialmente en conjuntos de datos desbalanceados donde cada clase es importante. # Save the trained model as pickle format with open(\"../model/xgboost_model.pkl\", \"wb\") as file: pickle.dump(xgb_model, file) # Save the trained model as a json file xgb_model.save_model(\"../model/xgboost_model.json\")","title":"SMOTE"},{"location":"xgboost/#matriz-de-confusion","text":"Analizar la matriz de confusi\u00f3n nos ayuda a comprender las clasificaciones incorrectas y a identificar \u00e1reas potenciales de mejora. # Plot XGBoost Confusion Matrix xgb_smote_model_cm = confusion_matrix(y_test, xgb_predictions) ConfusionMatrixDisplay(confusion_matrix = xgb_smote_model_cm).plot(cmap = \"PuBu\") plt.title(\"Matriz de confusi\u00f3n del modelo XGBoost\") plt.xlabel(\"Predicci\u00f3n\") plt.ylabel(\"Valor real\")","title":"Matriz de Confusi\u00f3n"},{"location":"xgboost/#importancia-de-las-caracteristicas","text":"Identificar las caracter\u00edsticas clave que impulsan las predicciones es fundamental para comprender c\u00f3mo el modelo toma decisiones. # Plot feature importance xgboost_feature_importance = plt.figure(figsize = (15, 3)) xgb.plot_importance(xgb_smote_model, importance_type = \"weight\", color = \"#74add1\", title = \"Importancia de caracter\u00edsticas de XGBoost\", xlabel = \"Peso\", ylabel = \"Caracter\u00edsticas\")","title":"Importancia de las caracter\u00edsticas"},{"location":"xgboost/#roc-auc","text":"Eval\u00faa el rendimiento del modelo de manera integral. # Binarize the output y_bin = label_binarize(y_test, classes = [0, 1, 2, 3, 4, 5, 6]) y_score = xgb_smote_model.predict_proba(X_test_scaled) # Compute ROC AUC for each class roc_auc = roc_auc_score(y_bin, y_score, average = \"macro\") print(f\"XGBoost Model ROC AUC Score: {roc_auc:.2f}\") # Binarize the output y_bin = label_binarize(y_test, classes = [0, 1, 2, 3, 4, 5, 6]) n_classes = y_bin.shape[1] # Compute ROC curve and ROC area for each class fpr = dict() tpr = dict() roc_auc = dict() for i in range(n_classes): fpr[i], tpr[i], _ = roc_curve(y_bin[:, i], xgb_smote_model.predict_proba(X_test_scaled)[:, i]) roc_auc[i] = auc(fpr[i], tpr[i]) # Plot the curves plt.figure(figsize = (11, 4)) for i in range(n_classes): plt.plot(fpr[i], tpr[i], color = colors[i], label = \"ROC curve of class {0} (area = {1:0.2f})\".format(i, roc_auc[i])) plt.plot([0, 1], [0, 1], \"k--\") plt.xlim([0.0, 1.0]) plt.ylim([0.0, 1.05]) plt.xlabel(\"Tasa de Falsos Positivos\") plt.ylabel(\"Tasa de Verdaderos Positivos\") plt.title(\"Curva Caracter\u00edstica de Operaci\u00f3n del Receptor (ROC)\") plt.legend(loc = \"lower right\")","title":"ROC AUC"},{"location":"xgboost/#curvas-de-aprendizaje","text":"Eval\u00faa el comportamiento del entrenamiento del modelo y detecta problemas de sesgo. # Plot the learning curves of the model plt.figure(figsize = (11, 4)) train_sizes, train_scores, test_scores = learning_curve(xgb_smote_model, X_resampled, y_resampled, cv = 5, n_jobs = -1, train_sizes = np.linspace(0.1, 1.0, 10)) train_scores_mean = train_scores.mean(axis = 1) test_scores_mean = test_scores.mean(axis = 1) plt.plot(train_sizes, train_scores_mean, color = \"#74add1\", label = \"Puntuaci\u00f3n de entrenamiento\") plt.plot(train_sizes, test_scores_mean, color = \"#f46d43\", label = \"Puntuaci\u00f3n de validaci\u00f3n cruzada\") plt.title(\"Curva de aprendizaje del modelo XGBoost\") plt.xlabel(\"Tama\u00f1o de entrenamiento\") plt.ylabel(\"Puntuaci\u00f3n\") plt.legend(loc = \"best\")","title":"Curvas de Aprendizaje"},{"location":"xgboost/#curva-de-precision-recall","text":"Eval\u00faa el rendimiento del modelo en t\u00e9rminos de precisi\u00f3n y recuperaci\u00f3n, especialmente \u00fatil en casos de clases desbalanceadas. # Plot the precision-recall curve plt.figure(figsize=(11, 4)) for i in range(n_classes): precision, recall, _ = precision_recall_curve(y_bin[:, i], xgb_smote_model.predict_proba(X_test_scaled)[:, i]) plt.plot(recall, precision, color = colors[i], label = f\"Curva de Precisi\u00f3n-Recuperaci\u00f3n de la clase {i}\") plt.xlabel(\"Recuperaci\u00f3n\") plt.ylabel(\"Precisi\u00f3n\") plt.title(\"Curva de Precisi\u00f3n-Recuperaci\u00f3n\") plt.legend(loc = \"lower left\")","title":"Curva de Precisi\u00f3n-Recall"}]}